{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24.08.05 머신러닝 학습 19회차: 베이지안 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎베이지안 최적화(Bayesian Optimization)\n",
    "\n",
    "1. 머신러닝에서 **하이퍼파라미터 튜닝**을 효율적으로 수행하기 위한 방법이다.\n",
    "\n",
    "2. 하이퍼파라미터 공간을 탐색하면서 모델의 성능을 최대화하는 하이퍼파라미터를 찾는 데 사용된다.\n",
    "\n",
    "3. 특히, 계산 비용이 높은 모델들에 유용하다.\n",
    "\n",
    "4. 일반적인 접근 방식은 **확률론적 모델**을 사용해 최적의 하이퍼파라미터를 예측하고 이를 기반으로 탐색하는 것이다.\n",
    "\n",
    "5. Surrogate Model과 Acquisition Function로 구성되며, 둘을 기반으로 판단한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎베이지안 최적화의 핵심 개념\n",
    "\n",
    "1. 하이퍼파라미터 공간 탐색\n",
    "\n",
    "    - 베이지안 최적화는 하이퍼파라미터 공간을 효율적으로 탐색한다.\n",
    "    \n",
    "    - 하이퍼파라미터 공간이란? 모델의 성능에 영향을 미치는 하이퍼파라미터들의 조합을 의미한다.\n",
    "\n",
    "2. 확률적 모델(Surrogate Model): 아래에 상세히 설명\n",
    "\n",
    "    - 보통 가우시안 프로세스(Gaussian Process) 또는 트리 기반 모델을 사용하여 하이퍼파라미터 공간을 모델링한다.\n",
    "    \n",
    "    - 이 모델은 주어진 하이퍼파라미터 조합에서의 성능을 예측한다.\n",
    "\n",
    "3. 획득 함수(Acquisition Function)\n",
    "    \n",
    "    - 베이지안 최적화에서 다음 샘플링 지점(다음으로 탐색할 하이퍼파라미터)을 선택하는 데 핵심적인 역할을 한다.\n",
    "    \n",
    "    - 탐색과 활용을 균형 있게 고려하여 최적화를 효율적으로 수행한다.\n",
    "    \n",
    "    - 대표적인 획득 함수로는 기대 개선(EI, Expected Improvement), 개선 확률(PI, Probability of Improvement), 그리고 하한 신뢰 구간(LCB, Lower Confidence Bound)이 있다.\n",
    "    \n",
    "    - 이들은 모두 서로게이트 모델이 예측한 정보를 바탕으로 최적의 샘플링 지점을 찾아내는 방법이다.\n",
    "\n",
    "    - 보통 예측 성능이 높을 것으로 기대되는 영역을 탐색한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎확률적 모델(Surrogate Model)\n",
    "\n",
    "이 모델은 주어진 데이터를 기반으로 고차원 함수의 형태를 추정하고, 이를 통해 최적화 과정을 더 효율적으로 수행한다.\n",
    "\n",
    "1. 목적 및 역할\n",
    "\n",
    "    - 비용 절감: 복잡한 함수나 시뮬레이션의 직접 평가가 비용이 많이 들거나 시간이 오래 걸릴 때, 서로게이트 모델은 이를 대신하여 빠르고 저렴하게 평가할 수 있다.\n",
    "\n",
    "    - 고차원 함수의 근사: 서로게이트 모델은 주어진 데이터 포인트를 사용하여 고차원 함수를 근사화하고, 이 근사화를 기반으로 최적화를 진행한다.\n",
    "\n",
    "    - 효율적인 탐색: 서로게이트 모델은 최적의 결과를 얻기 위해 탐색과 활용을 균형 있게 진행할 수 있도록 돕는다.\n",
    "\n",
    "2. 베이지안 최적화에서의 사용\n",
    "\n",
    "    - 하이퍼파라미터 튜닝: 머신러닝 모델의 하이퍼파라미터를 최적화할 때 서로게이트 모델을 사용하여 최적의 하이퍼파라미터를 효율적으로 찾는다.\n",
    "\n",
    "    - 획득 함수와 결합: 서로게이트 모델과 획득 함수(Acquisition Function)를 결합하여 다음 샘플링 포인트를 결정한다.\n",
    "\n",
    "3. 대표적인 서로게이트 모델:\n",
    "\n",
    "    - 가우시안 프로세스(Gaussian Processes): 베이지안 최적화에서 가장 많이 사용되는 모델로, 함수의 불확실성을 모델링하여 탐색과 활용의 균형을 잘 맞춘다.\n",
    "\n",
    "    - 랜덤 포레스트(Random Forests): 많은 데이터 포인트가 있는 경우 사용되며, 여러 결정 트리를 조합하여 예측의 안정성을 높인다.\n",
    "\n",
    "    - 신경망(Neural Networks): 고차원 데이터에서의 복잡한 패턴을 학습할 수 있으며, 최근에는 베이지안 신경망이 연구되고 있다.\n",
    "\n",
    "4. 획득 함수(Acquisition Function)\n",
    "\n",
    "    - Expected Improvement (EI): 현재 최적의 결과보다 더 나은 결과를 얻을 기대값을 계산한다.\n",
    "\n",
    "    - Probability of Improvement (PI): 현재 최적의 결과보다 더 나은 결과가 나올 확률을 계산한다.\n",
    "\n",
    "    - Lower Confidence Bound (LCB): 예측 평균과 불확실성(표준편차)의 조합을 사용하여 다음 샘플링 지점을 선택한다.\n",
    "\n",
    "5. 요약\n",
    "\n",
    "    - 서로게이트 모델은 직접 평가가 어려운 최적화 문제에서 효율적으로 함수를 근사화하여 최적화를 진행할 수 있도록 도와주는 모델이다.\n",
    "    \n",
    "    - 베이지안 최적화에서는 서로게이트 모델과 획득 함수를 결합하여 탐색과 활용의 균형을 맞추며 최적의 결과를 찾는다.\n",
    "    \n",
    "    - 대표적인 서로게이트 모델로는 가우시안 프로세스, 랜덤 포레스트, 신경망 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎베이지안 최적화 과정\n",
    "\n",
    "1. 초기 샘플링\n",
    "\n",
    "    - 하이퍼파라미터 공간에서 초기 데이터 포인트 몇 개의 샘플을 무작위로 선택하여 모델을 평가한다.\n",
    "    \n",
    "    - 이를 통해 초기 데이터를 수집한다.\n",
    "\n",
    "2. 서로게이트 모델(확률적 모델) 학습\n",
    "\n",
    "    - 수집된 초기 샘플링 데이터를 사용하여 서로게이트 모델을 학습시킨다.\n",
    "    \n",
    "    - 이 모델은 `하이퍼파라미터와 성능 간의 관계를 추정`한다.\n",
    "\n",
    "3. 획득 함수 최적화\n",
    "\n",
    "    - 획득 함수를 사용하여 `다음으로 탐색할 하이퍼파라미터를 선택`한다.\n",
    "    \n",
    "    - 이는 보통 서로게이트 모델의 예측을 기반으로 하며, 다음 샘플링 지점을 결정하게 된다.\n",
    "\n",
    "4. 모델 평가\n",
    "\n",
    "    - 선택된 하이퍼파라미터를 사용하여 실제 모델을 평가한다.\n",
    "    \n",
    "    - 그 결과를 다시 서로게이트 모델에 반영한다.\n",
    "\n",
    "5. 반복\n",
    "\n",
    "    - 2~4단계를 반복하여 최적의 하이퍼파라미터를 찾을 때까지 과정이 계속된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎베이지안 최적화의 장점\n",
    "\n",
    "1. 효율성: 적은 횟수의 평가로도 좋은 성능의 하이퍼파라미터를 찾을 수 있다.\n",
    "\n",
    "2. 계산 비용 절감: 계산 비용이 높은 모델에 특히 유용히다.\n",
    "\n",
    "3. 적응형 탐색: 서로게이트 모델과 획득 함수를 통해 하이퍼파라미터 공간을 효율적으로 탐색한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎베이지안 최적화의 활용\n",
    "\n",
    "1. 베이지안 최적화는 다양한 머신러닝 라이브러리에서 지원된다.\n",
    "\n",
    "2. 예를 들어, scikit-optimize의 BayesSearchCV, hyperopt, Optuna 등 여러 라이브러리에서 베이지안 최적화를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎간단 코드 실습\n",
    "\n",
    "1. `SVM(서포트 벡터 머신)` 모델의 `최적 하이퍼파라미터`를 `베이지안 최적화`를 통해 찾아보자.\n",
    "\n",
    "2. BayesSearchCV를 사용하여 여러 하이퍼파라미터 조합을 탐색하고, 성능을 반복적으로 평가하여 점진적으로 최적의 하이퍼파라미터를 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('C', 3987.340160817022), ('degree', 2), ('gamma', 0.0015447095152487998), ('kernel', 'poly')])\n"
     ]
    }
   ],
   "source": [
    "# pip install scikit-optimize 설치하기\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 샘플 데이터 불러오기\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 모델 및 베이지안 최적화 설정한다.\n",
    "opt = BayesSearchCV(  # 베이지안 최적화를 불러온다.\n",
    "    SVC(),  # SVM 모델을 정의한다.\n",
    "    {\n",
    "        'C': (1e-6, 1e+6, 'log-uniform'),  # 'C'하이퍼파라미터는 10−6에서 10 6사이의 로그 균등 분포에서 샘플링된다.\n",
    "        'gamma': (1e-6, 1e+1, 'log-uniform'), # 'gamma'는 10-6에서 10 1사이의 로그 균등 분포에서 샘플링된다.\n",
    "        'degree': (1, 8),  # 'degree' 하이퍼파라미터는 1에서 8 사이의 정수이다.\n",
    "        'kernel': ['linear', 'poly', 'rbf']  # kernel 하이퍼파라미터는 'linear', 'poly', 'rbf' 중 하나이다.\n",
    "    },\n",
    "    n_iter=32,  # 32번의 반복(iteration)을 수행하여 최적의 하이퍼파라미터 조합을 찾는다.\n",
    "    cv=3  # 3겹 교차 검증(cross-validation)을 사용한다.\n",
    ")\n",
    "\n",
    "# 학습 데이터와 라벨을 사용하여 베이지안 최적화 과정을 수행하여 최적의 하이퍼파라미터 찾는다.\n",
    "opt.fit(X, y)\n",
    "\n",
    "# 최적화 과정에서 찾은 최적의 하이퍼파라미터 값을 출력한다.\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎결론\n",
    "\n",
    "1. 베이지안 최적화는 효율적으로 하이퍼파라미터 튜닝을 수행하는 강력한 방법이다.\n",
    "\n",
    "2. 머신러닝 모델의 하이퍼파라미터 튜닝에서 이 방법을 적극적으로 활용한다면 모델의 성능을 극대화하는 데 큰 도움이 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 코드 실습(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 목표: 베이지안 최적화를 사용하여 SVM 모델의 최적 하이퍼파라미터를 찾고, 테스트 데이터에 대한 모델의 성능을 평가한다.\n",
    "\n",
    "2. 주의: 데이터를 표준화할 때, 훈련 데이터를 학습시키고 변환 & 테스트 데이터는 해당 모델을 바탕으로 변환만 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 설정\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from skopt import BayesSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "# 와인 데이터셋을 불러온다.\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 분리한다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 데이터를 표준화한다: 훈련 데이터를 학습시키고 변환, 테스트 데이터는 해당 모델을 바탕으로 변환만 한다.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  OrderedDict([('C', 0.08341564384216595), ('degree', 6), ('gamma', 3.389034515643755), ('kernel', 'linear')])\n",
      "Test Accuracy:  0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "# 베이지안 최적화를 사용하여 SVM 모델 최적의 하이퍼파라미터를 찾아낸다.\n",
    "opt = BayesSearchCV(\n",
    "    SVC(),\n",
    "    {\n",
    "        'C': (1e-6, 1e+6, 'log-uniform'),\n",
    "        'gamma': (1e-6, 1e+1, 'log-uniform'),\n",
    "        'degree': (1, 8),\n",
    "        'kernel': ['linear', 'poly', 'rbf']\n",
    "    },\n",
    "    n_iter=32,\n",
    "    cv=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 훈련 데이터의 최적의 하이퍼파라미터 찾는다.\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력한다.\n",
    "print(\"Best Parameters: \", opt.best_params_)\n",
    "\n",
    "# 테스트 데이터에 대한 모델 성능 평가\n",
    "accuracy = opt.score(X_test, y_test)\n",
    "print(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 참고 자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=w9D8ozS0oC4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
